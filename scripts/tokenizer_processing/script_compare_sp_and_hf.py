"""
EXECUTION: python script_compare_sp_and_hf.py
           --tokenizer_directory <tokenizer_directory>

PURPOSE: the script
         - loads the SP tokenizer from <output>/<tokenizer_directory>/model.model
         - loads the corresponding HF tokenizer from <output>/<tokenizer_directory>/tokenizer_vocab.json and [..]/tokenizer_merge.txt
           (Note: the latter can be created by script_create_merge_file_sp.py)
         - compares the two tokenizers (vocab & examples)
"""

import argparse
from os.path import join
import sentencepiece as spm
from tokenizers.implementations.sentencepiece_bpe import SentencePieceBPETokenizer
from tokenizers.normalizers import NFC

from os.path import abspath, dirname
import sys
BASE_DIR = abspath(dirname(dirname(dirname(abspath(__file__)))))
print(f">>> BASE_DIR: {BASE_DIR}")
sys.path.append(BASE_DIR)

from src.env import Env
from src.hardcoded.test_data import TEST_EXAMPLES
import json


def print_sp_vs_hf_for_i(_sp, _hf, _i):
    print()
    print(f"> i = {_i}")
    print(f"  --- SP ---")
    print(f"  sp.id_to_piece({_i}) = {_sp.id_to_piece(_i)}")
    print(f"  sp.decode([{_i}]) = {_sp.decode([_i])}")
    print(f"  repr(sp.decode([{_i}])) = {repr(_sp.decode([_i]))}")
    print(f"  --- HF ---")
    print(f"  hf.id_to_token({_i}) = {_hf.id_to_token(_i)}")
    print(f"  hf.decode([{_i}]) = {_hf.decode([_i])}")
    print(f"  repr(hf.decode([{_i}])) = {repr(_hf.decode([_i]))}")


def main(_args):
    # model_name = "180051_SP-uNone-d1-p1-w2-c1-f0-bf1-cc0.9999-x1-v64000_tokenizer2"
    model_name = _args.tokenizer_directory

    env = Env(".")
    model_file = join(env.output, model_name, "model.model")
    vocab_file = join(env.output, model_name, "tokenizer_vocab.json")
    merge_file = join(env.output, model_name, "tokenizer_merge.txt")

    sp = spm.SentencePieceProcessor(model_file=model_file)

    with open(vocab_file, "r") as f:
        vocab = json.load(f)
    with open(merge_file, "r") as f:
        merges = f.readlines()
        merges = [(elem.split()[0], elem.split()[1]) for elem in merges]

    special_tokens = [
        elem for elem in vocab.keys()
        if (elem.startswith("<|") and elem.endswith("|>"))
    ]
    byte_fallbacks = [
        elem for elem in vocab.keys()
        if (elem.startswith("<0x") and elem.endswith(">"))
    ]

    hf = SentencePieceBPETokenizer(vocab=vocab, merges=merges, add_prefix_space=True)
    hf.normalizer = NFC()
    hf.add_special_tokens(special_tokens)
    hf.add_special_tokens(byte_fallbacks)

    ##########################
    # 1. vocab
    ##########################
    print()
    print("========================")
    print("===== 1. VOCAB =========")
    print("========================")

    for i in [0, 1, 63423, 63999]:
        print_sp_vs_hf_for_i(sp, hf, i)

    unequal = list()
    for i in range(64000):
        if sp.id_to_piece(i) != hf.id_to_token(i) or sp.decode([i]) != hf.decode([i]) or repr(sp.decode([i])) != repr(hf.decode([i])):
            unequal.append(i)

    print()
    print(f">>> SP != HF: {unequal}")
    for j in [0, 1, -1]:
        print_sp_vs_hf_for_i(sp, hf, unequal[j])

    ##########################
    # 2. examples
    ##########################
    print()
    print("========================")
    print("===== 2. EXAMPLES ======")
    print("========================")
    examples = [
        "This is an example of text generated by a model. It is completely made up.",
        "Hello, my name is Ariel. Hej, mitt namn Ã¤r Ariel."
    ]
    examples += TEST_EXAMPLES
    for example in examples:
        sp_encoded = sp.encode(example)
        sp_decoded = sp.decode(sp_encoded)
        sp_decoded_elementwise = [sp.decode(elem) for elem in sp_encoded]

        hf_encoded = hf.encode(example).ids
        hf_decoded = hf.decode(hf_encoded)
        hf_decoded_elementwise = [hf.decode([elem]) for elem in hf_encoded]

        if sp_encoded == hf_encoded and sp_decoded == hf_decoded and sp_decoded_elementwise == hf_decoded_elementwise:
            equal = True
        else:
            equal = False
        if not equal:
            print()
            print(f"> example = {example}")
            print(f"  --- SP ---")
            print(f"  encoded (whole example) = {sp_encoded}")
            print(f"  decoded (whole example) = '{sp_decoded}'")
            print(f"  decoded (elementwise)   = {sp_decoded_elementwise}")
            print(f"  --- HF ---")
            print(f"  encoded (whole example) = {hf_encoded}")
            print(f"  decoded (whole example) = '{hf_decoded}'")
            print(f"  decoded (elementwise)   = {hf_decoded_elementwise}")
            print()
            print(f">>> SP == HF: {equal}")

    # print(sp)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--tokenizer_directory", type=str, required=True)
    _args = parser.parse_args()

    main(_args)

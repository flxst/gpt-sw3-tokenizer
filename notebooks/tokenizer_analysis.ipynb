{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4d2fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, isfile\n",
    "from typing import List\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from src.test_data import TEST_EXAMPLES\n",
    "\n",
    "\n",
    "from ipywidgets import interact, Checkbox\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import sentencepiece as spm\n",
    "\n",
    "from plots import plot_histogram, compare_vocab, plot_overview, plot_timelines, plot_overview_data, plot_vocab_size\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7967cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8308355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models() -> List[str]:\n",
    "    return [\n",
    "        elem for elem in sorted(os.listdir(OUTPUT_DIR)) \n",
    "        if not elem.startswith(\".\") and not elem.startswith(\"evaluation\")\n",
    "    ]\n",
    "    \n",
    "models = get_models()\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4468d1eb",
   "metadata": {},
   "source": [
    "# 1. Show examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ca158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\N{ANGSTROM SIGN}\", \"\\N{LATIN CAPITAL LETTER A WITH RING ABOVE}\", \"\\u0041\\u030A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = TEST_EXAMPLES + [\n",
    "    'Allmänna Allmänna',\n",
    "    \"<|endoftext|> test\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacff17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_hack(_decoded_elementwise):\n",
    "    \"\"\"\n",
    "    needs to be improved: \n",
    "    - should only be applied if add_prefix_space == True & add_whitespace_tokens == 24\n",
    "    - should only change an element if the next element is a non-whitespace-element\n",
    "    \"\"\"\n",
    "    return [\n",
    "        elem[:-1] \n",
    "        if set(elem) == {' '} \n",
    "        else elem\n",
    "        for elem in _decoded_elementwise \n",
    "    ]\n",
    "    # return \"\".join(decoded_elementwise_hack)\n",
    "\n",
    "def display(_example_decoded_per_token, show_linebreak = False):\n",
    "    newline = \"↩\\n\" if show_linebreak else \"↩\"\n",
    "    example_decoded_per_token = [\n",
    "            elem.replace(\"\\n\", newline).replace(\" \", \"-\")\n",
    "            for elem in _example_decoded_per_token\n",
    "        ]\n",
    "    \n",
    "    COLORS = [\"red\", \"blue\"]\n",
    "    for i, elem in enumerate(example_decoded_per_token):\n",
    "        print(colored(elem, COLORS[i%len(COLORS)]), end=\"\")\n",
    "    print()\n",
    "    print(f\"> {len(example_decoded_per_token)} tokens\")\n",
    "    print()\n",
    "\n",
    "def show_example_model(example, model, show_tokenization, verbose: bool = False):\n",
    "    _id = model.split(\"_\")[0]\n",
    "    \n",
    "    if isfile(join(OUTPUT_DIR, model, \"tokenizer.json\")):\n",
    "        library = \"HF\"\n",
    "        tokenizer_file = join(OUTPUT_DIR, model, \"tokenizer.json\")\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_file)\n",
    "        tokenizer_fast = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "        encoding = tokenizer_fast.encode(example)\n",
    "        example_encoded = tokenizer_fast.convert_ids_to_tokens(encoding)   \n",
    "        example_decoded = tokenizer_fast.decode(encoding)    \n",
    "        \n",
    "    elif isfile(join(OUTPUT_DIR, model, \"model.model\")):\n",
    "        library = \"SP\"\n",
    "        tokenizer_file = join(OUTPUT_DIR, model, \"model.model\")\n",
    "        sp = spm.SentencePieceProcessor(model_file=tokenizer_file)\n",
    "        encoding = sp.encode(example, out_type=int)\n",
    "        example_encoded = sp.encode(example, out_type=str)\n",
    "        example_decoded = sp.decode(example_encoded)\n",
    "\n",
    "    example_decoded_bytes = example_decoded.encode(\"utf-8\")\n",
    "\n",
    "    if library == \"HF\":\n",
    "        example_decoded_elementwise = [tokenizer_fast.decode(elem) for elem in encoding]\n",
    "    elif library == \"SP\":\n",
    "        example_decoded_elementwise = list()\n",
    "        idx_end = 0\n",
    "        for i, token in enumerate(example_encoded):\n",
    "            if i == 0 and token.startswith(\"▁\"):\n",
    "                _token = token[1:]\n",
    "            elif i > 0 and token.startswith(\"▁\"):\n",
    "                _token = token.replace(\"▁\", \" \")\n",
    "            else:\n",
    "                _token = token\n",
    "                \n",
    "            if _token.startswith(\"<\") and _token.endswith(\">\"):\n",
    "                _token = sp.decode(_token)\n",
    "            # print(i, token, _token)\n",
    "            idx_start = example_decoded[idx_end:].find(_token) + idx_end\n",
    "            idx_end = idx_start + len(_token)\n",
    "            # print(idx_start, idx_end)\n",
    "            # print()\n",
    "            example_decoded_elementwise.append(example_decoded[idx_start: idx_end])\n",
    "                \n",
    "    example_decoded_elementwise_hack = decode_hack(example_decoded_elementwise)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"============ {model}\")\n",
    "        print(f\"example: '{example}'\")\n",
    "        # if library == \"HF\":\n",
    "        #     print(f\"\\npre-tok: {tokenizer.pre_tokenizer.pre_tokenize_str(example)}\")\n",
    "        print(f\"\\nencoding: {encoding}\")\n",
    "        print(f\"\\nencoded: {example_encoded} --- {len(example_encoded)}\")\n",
    "        print(f\"\\ndecoded: '{example_decoded}'\")\n",
    "        # print(f\"\\ndecoded as bytes: {example_decoded_bytes}\")\n",
    "        print(f\"\\ndecoded elementwise: {example_decoded_elementwise}\")\n",
    "        print()\n",
    "    \n",
    "    if show_tokenization: \n",
    "        print(\"\\nencoded:\")\n",
    "        display(example_encoded)\n",
    "        print(\"\\ndecoded:\")\n",
    "        display(example_decoded_elementwise, show_linebreak=True)\n",
    "        # if library == \"HF\":\n",
    "        #     print(\"\\ndecoded + hack:\")\n",
    "        #     display(example_decoded_elementwise_hack, show_linebreak=True)\n",
    "        print(f\"\\ndecoded = original: {example == example_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d31a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_examples(example=test_examples, model=[\"ALL\"] + models, show_tokenization=True, verbose=False):\n",
    "    if model == \"ALL\":\n",
    "        for model in sorted(models):\n",
    "            show_example_model(example, model, show_tokenization, verbose)\n",
    "    else:\n",
    "        show_example_model(example, model, show_tokenization, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09394dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1130e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "\"ℌej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # ℌ, H --- ÅNGSTRÖM, Å, A+°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFC\n",
    "\"ℌej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # ℌ, H --- Å, Å, Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFKD\n",
    "\"Hej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # H, H --- A+°, A+°, A+°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFKC\n",
    "\"Hej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # H, H --- Å, Å, Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d910c",
   "metadata": {},
   "source": [
    "# 2. Subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a1ae3",
   "metadata": {},
   "source": [
    "### 2a. Subword Length Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75221f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_histogram(model_1=models, model_2=[None] + models, xlim=20, ylim=15000):\n",
    "    plot_histogram(model_1, model_2, xlim, ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f733c",
   "metadata": {},
   "source": [
    "### 2b. Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92205fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_compare_vocab(model_1=models, model_2=models, nr=30):\n",
    "    v, ex1, ex2 = compare_vocab(model_1, model_2, 1000000, 1000000)\n",
    "    print(v)\n",
    "    print()\n",
    "    print(\"=== only model 1 ===\")\n",
    "    print(ex1[:nr])\n",
    "    print()\n",
    "    print(\"=== only model 2 ===\")\n",
    "    print(ex2[:nr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e23f3f",
   "metadata": {},
   "source": [
    "### 2c. Vocabulary Size & Subword Length Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_vocab_size(model=models):\n",
    "    plot_vocab_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75086b27",
   "metadata": {},
   "source": [
    "# 3. Multilinguality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_multilinguality = [model for model in models if model.count(\"_3\") > 0]\n",
    "if len(models_multilinguality):\n",
    "    _core = list(set([\"_\".join(model.split(\"_\")[1:-1]) for model in models_multilinguality if model.endswith(\"da\")]))[0]\n",
    "    core = _core#.split(\"-v\")[0]\n",
    "    print(core)\n",
    "    # vocab = _core.split(\"-v\")[-1]\n",
    "    # print(vocab)\n",
    "    models_multilinguality = [model for model in models_multilinguality if core in model]\n",
    "    print(models_multilinguality)\n",
    "    models_multilinguality.sort(key = lambda x: x.split(\"_3\")[-1])\n",
    "    models_multilinguality = {model.split(\"_3\")[-1]: model for model in models_multilinguality}\n",
    "\n",
    "models_multilinguality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(models_multilinguality):\n",
    "    lang_complete = list(models_multilinguality.keys())\n",
    "    lang_all = [l for l in lang_complete if l.startswith(\"all\")]\n",
    "    lang_pure = [l for l in lang_complete if not l.startswith(\"all\")]\n",
    "\n",
    "    models_complete = {k: models_multilinguality[k] for k in lang_complete}\n",
    "    models_all = {k: models_multilinguality[k] for k in lang_all}\n",
    "    models_pure = {k: models_multilinguality[k] for k in lang_pure}\n",
    "else:\n",
    "    lang_complete, lang_all, lang_pure, models, models_all, models_pure = [[]]*6\n",
    "    \n",
    "lang_complete, lang_all, lang_pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e98962",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c794dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview_corpus(models_multilinguality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403f4a2",
   "metadata": {},
   "source": [
    "### 3a. Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(models_multilinguality):\n",
    "    plot_overview_data(models_pure.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eea739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(models_multilinguality):\n",
    "    plot_overview(models_pure.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ac703",
   "metadata": {},
   "source": [
    "### 3b. Intersection Matrix (Subword Length)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e907bb43",
   "metadata": {},
   "source": [
    "def get_intersection_matrix(subword_length_threshold = None, normalize = True, plot = True):\n",
    "\n",
    "    lang = list(models_multilinguality.keys())\n",
    "    N = len(lang)\n",
    "    intersection_matrix = np.zeros([N, N])\n",
    "\n",
    "    for i, j in product(range(N), range(N)):\n",
    "        model_1 = models_multilinguality[lang[i]]\n",
    "        model_2 = models_multilinguality[lang[j]]\n",
    "\n",
    "        v, _, _ = compare_vocab(model_1, model_2, subword_length_threshold)\n",
    "        # print(lang_1, lang_2, v[\"intersection\"])\n",
    "        intersection_matrix[i, j] = v[\"intersection\"]\n",
    "    \n",
    "    # print(lang)\n",
    "    # print(intersection_matrix)\n",
    "    if normalize:\n",
    "        for i, j in product(range(N), range(N)):\n",
    "            if i != j:\n",
    "                intersection_matrix[i, j] = intersection_matrix[i, j] / intersection_matrix[i, i]\n",
    "        for i in range(N):\n",
    "            intersection_matrix[i, i] = 1.0\n",
    "    # print(intersection_matrix)\n",
    "    \n",
    "    if plot:\n",
    "        ax = sns.heatmap(intersection_matrix, \n",
    "                         xticklabels=lang,\n",
    "                         yticklabels=lang,\n",
    "                         cmap=\"binary\",\n",
    "                         vmin=0,\n",
    "                         annot=True,\n",
    "        )\n",
    "    else:\n",
    "        return intersection_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14164e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc25996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10272124",
   "metadata": {},
   "source": [
    "### 3c. Evaluation #1: Vocabulary Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a86d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection(lang_1, lang_2, vocab_1, vocab_2):\n",
    "    model_1 = models_multilinguality[lang_1]\n",
    "    model_2 = models_multilinguality[lang_2]\n",
    "    v, _, _ = compare_vocab(model_1, model_2, vocab_1, vocab_2)\n",
    "    return v[\"intersection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342be9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(models_multilinguality):\n",
    "    get_intersection('all-a1.0', 'da', 10000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = [10000, 20000, 30000, 40000, 51200, 64000, 80000, 96000, 112000, 128000]\n",
    "VOCAB_1 = VOCAB\n",
    "VOCAB_2 = VOCAB\n",
    "\n",
    "# VOCAB_1 = [50000, 100000, 150000, 200000, 250000]\n",
    "# VOCAB_2 = [100, 1000, 10000, 20000, 30000, 40000, 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(models_multilinguality):\n",
    "    intersections = {\n",
    "        lang_1: {\n",
    "            lang_2: {\n",
    "                vocab_1: {\n",
    "                    vocab_2: get_intersection(lang_1, lang_2, vocab_1, vocab_2)\n",
    "                    for vocab_2 in VOCAB_2\n",
    "                }\n",
    "                for vocab_1 in VOCAB_1\n",
    "            }\n",
    "            for lang_2 in lang_complete\n",
    "        }\n",
    "        for lang_1 in lang_all\n",
    "    }\n",
    "else:\n",
    "    intersections = None\n",
    "\n",
    "# intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(models_multilinguality):\n",
    "    timelines_abs = {\n",
    "        lang_1: {\n",
    "            vocab_2: {\n",
    "                lang_2: \n",
    "                [intersections[lang_1][lang_2][vocab_1][vocab_2] for vocab_1 in VOCAB_1]\n",
    "                for lang_2 in lang_pure\n",
    "            }\n",
    "            for vocab_2 in VOCAB_2\n",
    "        }\n",
    "        for lang_1 in lang_all\n",
    "    }\n",
    "else:\n",
    "    timelines_abs = None\n",
    "    \n",
    "# timelines_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37fbd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(models_multilinguality):\n",
    "    timelines_rel = {\n",
    "        lang_1: {\n",
    "            vocab_2: {\n",
    "                lang_2: \n",
    "                [intersections[lang_1][lang_2][vocab_1][vocab_2]/intersections[lang_1][lang_1][vocab_2][vocab_2] for vocab_1 in VOCAB_1]\n",
    "                for lang_2 in lang_pure\n",
    "            }\n",
    "            for vocab_2 in VOCAB_2\n",
    "        }\n",
    "        for lang_1 in lang_all\n",
    "    }\n",
    "else:\n",
    "    timelines_rel = None\n",
    "    \n",
    "# timelines_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e150846",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b9e5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_timelines(tokenizer=lang_all, vocab_size=VOCAB_2):\n",
    "    if tokenizer is not None:\n",
    "        lang_1 = tokenizer\n",
    "        vocab_2 = vocab_size\n",
    "        t_abs = timelines_abs[lang_1][vocab_2]\n",
    "        t_rel = timelines_rel[lang_1][vocab_2]\n",
    "\n",
    "        plot_timelines(\n",
    "            VOCAB_1,\n",
    "            vocab_2,\n",
    "            [t_abs, t_rel],\n",
    "            lang_pure, \n",
    "            ylim=[1.1*100000, 1.1],\n",
    "            ylabel=[\"absolute\", \"relative\"], \n",
    "            title=[\"Coverage of single-language tokenizer vocabulary\"]*2,\n",
    "        )\n",
    "    else:\n",
    "        print(\"> lang_all is []\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cabd27f",
   "metadata": {},
   "source": [
    "### 3d. Evaluation #2: unk_rate & closeness_to_character_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3cbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_results():\n",
    "    evaluation_dir = join(OUTPUT_DIR, \"evaluation\")\n",
    "    results = [elem.split(\"results_\")[-1].split(\".json\")[0] for elem in sorted(os.listdir(evaluation_dir))]\n",
    "    return results\n",
    "\n",
    "list_of_results = get_list_of_results()\n",
    "list_of_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763bf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_results(_result):\n",
    "    _results_path = join(OUTPUT_DIR, \"evaluation\", f\"results_{_result}.json\")\n",
    "    with open(_results_path, \"r\") as file:\n",
    "        r = json.load(file)\n",
    "    return r\n",
    "\n",
    "results = read_results('all-a1.0')\n",
    "if 0:\n",
    "    results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bf_cc_from_results(_results):\n",
    "    models = list(set(_results.keys()))\n",
    "    bfs = list(set([model.split(\"-bf\")[1].split(\"-cc\")[0] for model in models]))\n",
    "    ccs = list(set([model.split(\"-cc\")[1].split(\"-x\")[0] for model in models]))\n",
    "    return bfs, ccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f845322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_parameters_from_results(_bf, _cc, _results):\n",
    "    models = list(set(_results.keys()))\n",
    "    vocabs = sorted(list(set([int(model.split(\"-v\")[1].split(\"_\")[0]) for model in models])))\n",
    "    vocabs_model = {\n",
    "        vocab: [\n",
    "            model \n",
    "            for model in models \n",
    "            if f\"-bf{_bf}\" in model\n",
    "            and f\"-cc{_cc}\" in model\n",
    "            and f\"-v{vocab}_\" in model\n",
    "        ][0]\n",
    "        for vocab in vocabs\n",
    "    }\n",
    "    files = list(_results[models[0]].keys())\n",
    "    \n",
    "    languages = [file.split(\"/\")[-1].split(\".json\")[0].split(\"_\")[1] for file in files]  # WORKS ONLY FOR 'wiki_??_t1p'!!! \n",
    "    languages_files = {k: v for k, v in zip(languages, files)}\n",
    "    \n",
    "    if 0:\n",
    "        print(bfs)\n",
    "        print(ccs)\n",
    "        print()\n",
    "        print(vocabs)\n",
    "        print(vocabs_model)\n",
    "        print(files)\n",
    "        print(languages)\n",
    "    \n",
    "    return vocabs, vocabs_model, files, languages, languages_files\n",
    " \n",
    "if 0:\n",
    "    bfs, ccs = retrieve_bf_cc_from_results(results)\n",
    "    vocabs, vocabs_model, files, languages, languages_files = retrieve_parameters_from_results(bfs[0], ccs[0], results)\n",
    "    print(bfs)\n",
    "    print(ccs)\n",
    "    print()\n",
    "    print(vocabs)\n",
    "    print(vocabs_model)\n",
    "    print(files)\n",
    "    print(languages)\n",
    "    print(languages_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abf035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_2(_unk_rate, _ctcl, _vocabs, _languages, _ymin, _ymax):\n",
    "    import matplotlib.pyplot as plt\n",
    "    colors = {\"da\": \"r\", \"en\": \"g\", \"is\": \"b\", \"no\": \"purple\", \"sv\": \"orange\"}\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    for language in _languages:\n",
    "        ax[0].plot(_vocabs, _unk_rate[language], linestyle=None, marker=\"s\", color=colors[language], label=language)\n",
    "        ax[1].plot(_vocabs, _ctcl[language], linestyle=None, marker=\"s\", color=colors[language], label=language)\n",
    "    for i in range(2):\n",
    "        ax[i].set_xlim([0, 150000])\n",
    "        ax[i].set_ylim([_ymin, _ymax])\n",
    "        ax[i].legend()\n",
    "    ax[0].set_title(\"unknown rate (lower = better)\")\n",
    "    ax[1].set_title(\"closeness to character level (lower = better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_evaluation_2(result=list_of_results):\n",
    "    r = read_results(result)\n",
    "    bfs, ccs = retrieve_bf_cc_from_results(r)\n",
    "    \n",
    "    @interact\n",
    "    def show_evaluation_2_detail(bf=bfs, cc=ccs, ymin=0.0, ymax=1.0):\n",
    "        vocabs, vocabs_models, files, languages, languages_files = retrieve_parameters_from_results(bf, cc, r)\n",
    "        # print(bf, cc, result)\n",
    "        results_filtered = {k: v for k, v in r.items() if f\"-bf{bf}-cc{cc}\" in k}\n",
    "        # print()\n",
    "        # print(results_filtered)\n",
    "        # print()\n",
    "        \n",
    "        unk_rate = {\n",
    "            language: [\n",
    "                results_filtered[vocabs_models[vocab]][languages_files[language]][\"unk_rate\"]\n",
    "                for vocab in vocabs\n",
    "            ]\n",
    "            for language in languages\n",
    "        }\n",
    "        closeness_to_character_level = {\n",
    "            language: [\n",
    "                results_filtered[vocabs_models[vocab]][languages_files[language]][\"closeness_to_character_level\"]\n",
    "                for vocab in vocabs\n",
    "            ]\n",
    "            for language in languages\n",
    "        }\n",
    "        # print(unk_rate)\n",
    "        # print(closeness_to_character_level)\n",
    "\n",
    "        plot_evaluation_2(unk_rate, closeness_to_character_level, vocabs, languages, ymin, ymax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-gpt-sw3-tokenizer",
   "language": "python",
   "name": "venv-gpt-sw3-tokenizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

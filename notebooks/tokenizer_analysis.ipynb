{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13956492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, isfile\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from src.test_data import TEST_EXAMPLES\n",
    "\n",
    "\n",
    "from ipywidgets import interact, Checkbox\n",
    "\n",
    "from plots import get_models_in_output_dir\n",
    "from plots import tokenize_hf\n",
    "from plots import tokenize_sp\n",
    "from plots import display\n",
    "from plots import decode_hack\n",
    "\n",
    "from plots import get_models_multilinguality\n",
    "from plots import split_models_multilinguality\n",
    "from plots import get_intersection\n",
    "from plots import get_intersections\n",
    "\n",
    "from plots import plot_histogram, compare_vocab, plot_overview, plot_timelines, plot_overview_data, plot_vocab_size\n",
    "\n",
    "from plots import get_list_of_results\n",
    "from plots import read_results\n",
    "from plots import retrieve_bf_cc_from_results\n",
    "from plots import retrieve_parameters_from_results\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbdc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79deb7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBDIRS = [\"multilinguality\", \"p_w\", \"bf_cc\", \"min_frequency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9a200",
   "metadata": {},
   "source": [
    "# 0. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ba6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models_in_output_dir(SUBDIRS)\n",
    "for subdir in SUBDIRS:\n",
    "    for model in models[subdir]:\n",
    "        print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeac9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf26a1d",
   "metadata": {},
   "source": [
    "# 1. Tokenization examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6032cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_model(example: str, \n",
    "                       model: str, \n",
    "                       show_tokenization: bool = True, \n",
    "                       verbose: bool = False):\n",
    "    \n",
    "    _id = model.split(\"_\")[0]\n",
    "    \n",
    "    if isfile(join(OUTPUT_DIR, model, \"tokenizer.json\")):\n",
    "        texample = tokenize_hf(model, example)\n",
    "    elif isfile(join(OUTPUT_DIR, model, \"model.model\")):\n",
    "        texample = tokenize_sp(model, example)\n",
    "    else:\n",
    "        raise Exception(f\"ERROR! could not find tokenizer for model = {model}.\")\n",
    "    assert \"\".join(texample['de-tokenized_elementwise']) == texample['de-tokenized'], f\"ERROR de-tokenized elementwise!\"\n",
    "    \n",
    "    print(f\"\\n============ {model}\")\n",
    "    if verbose:\n",
    "        print(f\"example: '{example}'\")\n",
    "        print(f\"\\nencoded: {texample['encoded']}\")\n",
    "        print(f\"\\ntokenized: {texample['tokenized']} --- {len(texample['tokenized'])}\")\n",
    "        print(f\"\\nde-tokenized: '{texample['de-tokenized']}'\")\n",
    "        print(f\"\\nde-tokenized elementwise: {texample['de-tokenized_elementwise']}\")\n",
    "        print()\n",
    "    \n",
    "    if show_tokenization: \n",
    "        print(\"\\ntokenized:\")\n",
    "        display(texample['tokenized'])\n",
    "        print(\"\\nde-tokenized:\")\n",
    "        display(texample['de-tokenized_elementwise'], \n",
    "                show_linebreak=True, \n",
    "                equal_to_original=example == texample['de-tokenized'])\n",
    "        if 'de-tokenized_elementwise_hack' in texample.keys():\n",
    "            print(\"\\ndecoded + hack:\")\n",
    "            display(texample['de-tokenized_elementwise_hack'], show_linebreak=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23037e12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_examples(example=TEST_EXAMPLES, \n",
    "                  model=models['all'], \n",
    "                  show_tokenization=True, \n",
    "                  verbose=False):\n",
    "    show_example_model(example, model, show_tokenization, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb46de",
   "metadata": {},
   "source": [
    "# 2. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd225238",
   "metadata": {},
   "source": [
    "### 2a. Subword Length Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb788658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_histogram(model_1=models['all'], model_2=[None] + models['all'], xlim=20, ylim=15000):\n",
    "    plot_histogram(model_1, model_2, xlim, ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e9fda",
   "metadata": {},
   "source": [
    "### 2b. Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8d43f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_compare_vocab(model_1=models['all'], model_2=models['all'], nr=30):\n",
    "    v, ex1, ex2 = compare_vocab(model_1, model_2, 1000000, 1000000)\n",
    "    print(v)\n",
    "    print()\n",
    "    print(\"=== only model 1 ===\")\n",
    "    print(ex1[:nr])\n",
    "    print()\n",
    "    print(\"=== only model 2 ===\")\n",
    "    print(ex2[:nr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c2b03",
   "metadata": {},
   "source": [
    "### 2c. Min Frequency: Vocabulary Size & Subword Length Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca51966",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_vocab_size(model=models['min_frequency']):\n",
    "    if model:\n",
    "        plot_vocab_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed59300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de7d94",
   "metadata": {},
   "source": [
    "# 3. Vocab Size & Multilinguality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_multilinguality = get_models_multilinguality(models['multilinguality'], verbose=False)\n",
    "models_multilinguality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = split_models_multilinguality(models_multilinguality)\n",
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview_corpus(models_multilinguality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a61f2",
   "metadata": {},
   "source": [
    "### 3a. Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(models_multilinguality):\n",
    "    plot_overview_data(ml[\"models_pure\"].values(), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa8c1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(models_multilinguality):\n",
    "    plot_overview(ml[\"models_pure\"].values(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640fa23",
   "metadata": {},
   "source": [
    "### 3b. Evaluation #1: Vocabulary Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39deb8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = [10000, 20000, 30000, 40000, 51200, 64000, 80000, 96000, 112000, 128000]\n",
    "vocabs_1 = vocabs\n",
    "vocabs_2 = vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelines = get_intersections(models_multilinguality, ml, vocabs_1, vocabs_2)\n",
    "\n",
    "print(type(timelines))\n",
    "print(list(timelines.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60922ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_evaluation_1(tokenizer=ml['lang_all'], vocab_size=vocabs_2, absolute=[False, True]):\n",
    "    if tokenizer is not None:\n",
    "        lang_1 = tokenizer\n",
    "        vocab_2 = vocab_size\n",
    "        t_abs = timelines['abs'][lang_1][vocab_2]\n",
    "        t_rel = timelines['rel'][lang_1][vocab_2]\n",
    "\n",
    "        if absolute:\n",
    "            plot_timelines(\n",
    "                vocabs_1,\n",
    "                vocab_2,\n",
    "                [t_rel, t_abs],\n",
    "                ml['lang_pure'], \n",
    "                ylim=[1.1, 1.1*100000],\n",
    "                ylabel=[\"relative\", \"absolute\"], \n",
    "                title=[\"Coverage of single-language tokenizer vocabulary\"]*2,\n",
    "            )\n",
    "        else:\n",
    "            plot_timelines(\n",
    "                vocabs_1,\n",
    "                vocab_2,\n",
    "                [t_rel],\n",
    "                ml['lang_pure'], \n",
    "                ylim=[1.1],\n",
    "                ylabel=[\"relative\"], \n",
    "                title=[\"Coverage of single-language tokenizer vocabulary\"],\n",
    "            )\n",
    "    else:\n",
    "        print(\"> lang_all is []\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98a0de",
   "metadata": {},
   "source": [
    "### 3c. Evaluation #2: unk_rate & closeness_to_character_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_2(_unk_rate, _ctcl, _vocabs, _languages, _ymin, _ymax):\n",
    "    import matplotlib.pyplot as plt\n",
    "    colors = {\"da\": \"r\", \"en\": \"g\", \"is\": \"b\", \"no\": \"purple\", \"sv\": \"orange\"}\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    for language in _languages:\n",
    "        ax[0].plot(_vocabs, _unk_rate[language], linestyle=None, marker=\"s\", color=colors[language], label=language)\n",
    "        ax[1].plot(_vocabs, _ctcl[language], linestyle=None, marker=\"s\", color=colors[language], label=language)\n",
    "    for i in range(2):\n",
    "        ax[i].set_xlim([0, 150000])\n",
    "        ax[i].set_ylim([_ymin, _ymax])\n",
    "        ax[i].legend()\n",
    "    ax[0].set_title(\"unknown rate (lower = better)\")\n",
    "    ax[1].set_title(\"closeness to character level (lower = better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e1289",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_evaluation_2(result=get_list_of_results()):\n",
    "    r = read_results(result)\n",
    "    bfs, ccs = retrieve_bf_cc_from_results(r)\n",
    "    \n",
    "    @interact\n",
    "    def show_evaluation_2_detail(bf=bfs, cc=ccs, ymin=0.0, ymax=1.0):\n",
    "        vocabs, vocabs_models, files, languages, languages_files = retrieve_parameters_from_results(bf, cc, r, verbose=False)\n",
    "        # print(bf, cc, result)\n",
    "        results_filtered = {k: v for k, v in r.items() if f\"-bf{bf}-cc{cc}\" in k}\n",
    "        # print()\n",
    "        # print(results_filtered)\n",
    "        # print()\n",
    "        \n",
    "        unk_rate = {\n",
    "            language: [\n",
    "                results_filtered[vocabs_models[vocab]][languages_files[language]][\"unk_rate\"]\n",
    "                for vocab in vocabs\n",
    "            ]\n",
    "            for language in languages\n",
    "        }\n",
    "        closeness_to_character_level = {\n",
    "            language: [\n",
    "                results_filtered[vocabs_models[vocab]][languages_files[language]][\"closeness_to_character_level\"]\n",
    "                for vocab in vocabs\n",
    "            ]\n",
    "            for language in languages\n",
    "        }\n",
    "        # print(unk_rate)\n",
    "        # print(closeness_to_character_level)\n",
    "\n",
    "        plot_evaluation_2(unk_rate, closeness_to_character_level, vocabs, languages, ymin, ymax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-gpt-sw3-tokenizer",
   "language": "python",
   "name": "venv-gpt-sw3-tokenizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

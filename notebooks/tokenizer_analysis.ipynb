{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202dadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from typing import List\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from src.test_data import TEST_EXAMPLES\n",
    "\n",
    "\n",
    "from ipywidgets import interact, Checkbox\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from plots import plot_histogram, compare_vocab, plot_overview, plot_timelines, plot_overview_data, plot_vocab_size\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5df237",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models() -> List[str]:\n",
    "    return [elem for elem in sorted(os.listdir(OUTPUT_DIR)) if not elem.startswith(\".\")]\n",
    "    \n",
    "models = get_models()\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b3f74",
   "metadata": {},
   "source": [
    "# 1. Show examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254dd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\N{ANGSTROM SIGN}\", \"\\N{LATIN CAPITAL LETTER A WITH RING ABOVE}\", \"\\u0041\\u030A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0298c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = TEST_EXAMPLES + [\n",
    "    'Allmänna Allmänna',\n",
    "    \"<|endoftext|> test\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_hack(_decoded_elementwise):\n",
    "    \"\"\"\n",
    "    needs to be improved: \n",
    "    - should only be applied if add_prefix_space == True & add_whitespace_tokens == 24\n",
    "    - should only change an element if the next element is a non-whitespace-element\n",
    "    \"\"\"\n",
    "    return [\n",
    "        elem[:-1] \n",
    "        if set(elem) == {' '} \n",
    "        else elem\n",
    "        for elem in _decoded_elementwise \n",
    "    ]\n",
    "    # return \"\".join(decoded_elementwise_hack)\n",
    "\n",
    "def display(_example_decoded_per_token):\n",
    "    example_decoded_per_token = [\n",
    "            elem.replace(\"\\n\", \"↩\\n\").replace(\" \", \"-\") \n",
    "            for elem in _example_decoded_per_token\n",
    "        ]\n",
    "    \n",
    "    COLORS = [\"red\", \"blue\"]\n",
    "    for i, elem in enumerate(example_decoded_per_token):\n",
    "        print(colored(elem, COLORS[i%len(COLORS)]), end=\"\")\n",
    "    print()\n",
    "    print(f\"> {len(example_decoded_per_token)} tokens\")\n",
    "    print()\n",
    "\n",
    "def show_example_model(example, model, show_tokenization, verbose: bool = False):\n",
    "    _id = model.split(\"_\")[0]\n",
    "    tokenizer_file = join(OUTPUT_DIR, model, \"tokenizer.json\")\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_file)\n",
    "    tokenizer_fast = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "    encoding = tokenizer_fast.encode(example)\n",
    "    example_encoded = tokenizer_fast.convert_ids_to_tokens(encoding)   \n",
    "    example_decoded = tokenizer_fast.decode(encoding)    \n",
    "    example_decoded_bytes = example_decoded.encode(\"utf-8\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"============ {model}\")\n",
    "        print(f\"example: '{example}'\")\n",
    "        print(f\"\\npre-tok: {tokenizer.pre_tokenizer.pre_tokenize_str(example)}\")\n",
    "        # print(encoding)\n",
    "        print(f\"\\nencoded: {example_encoded} --- {len(example_encoded)}\")\n",
    "        print(f\"\\ndecoded: '{example_decoded}'\")\n",
    "        print(f\"\\ndecoded as bytes: {example_decoded_bytes}\")\n",
    "        print()\n",
    "    \n",
    "    example_decoded_elementwise = [tokenizer_fast.decode(elem) for elem in encoding]\n",
    "    example_decoded_elementwise_hack = decode_hack(example_decoded_elementwise)\n",
    "    \n",
    "    if show_tokenization: \n",
    "        print(\"\\ndecoded:\")\n",
    "        display(example_decoded_elementwise)\n",
    "        print(\"\\ndecoded + hack:\")\n",
    "        display(example_decoded_elementwise_hack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebed99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_examples(example=test_examples, model=[\"ALL\"] + models, show_tokenization=True, verbose=False):\n",
    "    if model == \"ALL\":\n",
    "        for model in sorted(models):\n",
    "            show_example_model(example, model, show_tokenization, verbose)\n",
    "    else:\n",
    "        show_example_model(example, model, show_tokenization, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "\"ℌej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # ℌ, H --- ÅNGSTRÖM, Å, A+°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFC\n",
    "\"ℌej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # ℌ, H --- Å, Å, Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFKD\n",
    "\"Hej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # H, H --- A+°, A+°, A+°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFKC\n",
    "\"Hej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # H, H --- Å, Å, Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff391a",
   "metadata": {},
   "source": [
    "# 2. Subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eafa64",
   "metadata": {},
   "source": [
    "### 2a. Subword Length Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e487c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_histogram(model_1=models, model_2=[None] + models, xlim=20, ylim=15000):\n",
    "    plot_histogram(model_1, model_2, xlim, ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba8e574",
   "metadata": {},
   "source": [
    "### 2b. Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b279d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_compare_vocab(model_1=models, model_2=models, nr=5):\n",
    "    v, ex1, ex2 = compare_vocab(model_1, model_2, 1000000, 1000000)\n",
    "    print(v)\n",
    "    print()\n",
    "    print(\"=== only model 1 ===\")\n",
    "    print(ex1[:nr])\n",
    "    print()\n",
    "    print(\"=== only model 2 ===\")\n",
    "    print(ex2[:nr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb6f67",
   "metadata": {},
   "source": [
    "### 2c. Vocabulary Size & Subword Length Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec09cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_vocab_size(model=models):\n",
    "    plot_vocab_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d8892",
   "metadata": {},
   "source": [
    "# 3. Multilinguality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9fd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_multilinguality = [model for model in models if model.count(\"_3\") > 0]\n",
    "core = list(set([\"_\".join(model.split(\"_\")[1:-1]) for model in models_multilinguality if model.endswith(\"da\")]))[0]\n",
    "print(core)\n",
    "models_multilinguality = [model for model in models_multilinguality if core in model]\n",
    "models_multilinguality.sort(key = lambda x: x.split(\"_3\")[-1])\n",
    "models_multilinguality = {model.split(\"_3\")[-1]: model for model in models_multilinguality}\n",
    "models_multilinguality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = list(models_multilinguality.keys())\n",
    "lang_all = [l for l in lang if l.startswith(\"all\")]\n",
    "lang_pure = [l for l in lang if not l.startswith(\"all\")]\n",
    "\n",
    "models = {k: models_multilinguality[k] for k in lang}\n",
    "models_all = {k: models_multilinguality[k] for k in lang_all}\n",
    "models_pure = {k: models_multilinguality[k] for k in lang_pure}\n",
    "\n",
    "lang, lang_all, lang_pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52895c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview_corpus(models_multilinguality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719f3ee",
   "metadata": {},
   "source": [
    "### 3a. Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb9f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overview_data(models_pure.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1089d61a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_overview(models_pure.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b488f",
   "metadata": {},
   "source": [
    "### 3b. Intersection Matrix (Subword Length)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0325261a",
   "metadata": {},
   "source": [
    "def get_intersection_matrix(subword_length_threshold = None, normalize = True, plot = True):\n",
    "\n",
    "    lang = list(models_multilinguality.keys())\n",
    "    N = len(lang)\n",
    "    intersection_matrix = np.zeros([N, N])\n",
    "\n",
    "    for i, j in product(range(N), range(N)):\n",
    "        model_1 = models_multilinguality[lang[i]]\n",
    "        model_2 = models_multilinguality[lang[j]]\n",
    "\n",
    "        v, _, _ = compare_vocab(model_1, model_2, subword_length_threshold)\n",
    "        # print(lang_1, lang_2, v[\"intersection\"])\n",
    "        intersection_matrix[i, j] = v[\"intersection\"]\n",
    "    \n",
    "    # print(lang)\n",
    "    # print(intersection_matrix)\n",
    "    if normalize:\n",
    "        for i, j in product(range(N), range(N)):\n",
    "            if i != j:\n",
    "                intersection_matrix[i, j] = intersection_matrix[i, j] / intersection_matrix[i, i]\n",
    "        for i in range(N):\n",
    "            intersection_matrix[i, i] = 1.0\n",
    "    # print(intersection_matrix)\n",
    "    \n",
    "    if plot:\n",
    "        ax = sns.heatmap(intersection_matrix, \n",
    "                         xticklabels=lang,\n",
    "                         yticklabels=lang,\n",
    "                         cmap=\"binary\",\n",
    "                         vmin=0,\n",
    "                         annot=True,\n",
    "        )\n",
    "    else:\n",
    "        return intersection_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c55ee8",
   "metadata": {},
   "source": [
    "### 3c. Intersection Timeline (Subword Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection(lang_1, lang_2, vocab_1, vocab_2):\n",
    "    model_1 = models_multilinguality[lang_1]\n",
    "    model_2 = models_multilinguality[lang_2]\n",
    "    v, _, _ = compare_vocab(model_1, model_2, vocab_1, vocab_2)\n",
    "    return v[\"intersection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb731203",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_intersection('all-a1.0', 'da', 10000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = [10000, 20000, 30000, 40000, 51200, 64000, 80000, 96000, 112000, 128000]\n",
    "VOCAB_1 = VOCAB\n",
    "VOCAB_2 = VOCAB\n",
    "\n",
    "# VOCAB_1 = [50000, 100000, 150000, 200000, 250000]\n",
    "# VOCAB_2 = [100, 1000, 10000, 20000, 30000, 40000, 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections = {\n",
    "    lang_1: {\n",
    "        lang_2: {\n",
    "            vocab_1: {\n",
    "                vocab_2: get_intersection(lang_1, lang_2, vocab_1, vocab_2)\n",
    "                for vocab_2 in VOCAB_2\n",
    "            }\n",
    "            for vocab_1 in VOCAB_1\n",
    "        }\n",
    "        for lang_2 in lang\n",
    "    }\n",
    "    for lang_1 in lang_all\n",
    "}\n",
    "\n",
    "# intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelines_abs = {\n",
    "    lang_1: {\n",
    "        vocab_2: {\n",
    "            lang_2: \n",
    "            [intersections[lang_1][lang_2][vocab_1][vocab_2] for vocab_1 in VOCAB_1]\n",
    "            for lang_2 in lang_pure\n",
    "        }\n",
    "        for vocab_2 in VOCAB_2\n",
    "    }\n",
    "    for lang_1 in lang_all\n",
    "}\n",
    "# timelines_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d08f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelines_rel = {\n",
    "    lang_1: {\n",
    "        vocab_2: {\n",
    "            lang_2: \n",
    "            [intersections[lang_1][lang_2][vocab_1][vocab_2]/intersections[lang_1][lang_1][vocab_2][vocab_2] for vocab_1 in VOCAB_1]\n",
    "            for lang_2 in lang_pure\n",
    "        }\n",
    "        for vocab_2 in VOCAB_2\n",
    "    }\n",
    "    for lang_1 in lang_all\n",
    "}\n",
    "# timelines_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f657e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e877b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_timelines(tokenizer=lang_all, vocab_size=VOCAB_2):\n",
    "    lang_1 = tokenizer\n",
    "    vocab_2 = vocab_size\n",
    "    t_abs = timelines_abs[lang_1][vocab_2]\n",
    "    t_rel = timelines_rel[lang_1][vocab_2]\n",
    "    \n",
    "    plot_timelines(\n",
    "        VOCAB_1,\n",
    "        vocab_2,\n",
    "        [t_abs, t_rel],\n",
    "        lang_pure, \n",
    "        ylim=[1.1*100000, 1.1],\n",
    "        ylabel=[\"absolute\", \"relative\"], \n",
    "        title=[\"Coverage of single-language tokenizer vocabulary\"]*2,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-gpt-sw3-tokenizer",
   "language": "python",
   "name": "venv-gpt-sw3-tokenizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

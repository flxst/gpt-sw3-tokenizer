{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a5abb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, isfile\n",
    "from typing import List\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from src.test_data import TEST_EXAMPLES\n",
    "\n",
    "\n",
    "from ipywidgets import interact, Checkbox\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import sentencepiece as spm\n",
    "\n",
    "from plots import plot_histogram, compare_vocab, plot_overview, plot_timelines, plot_overview_data, plot_vocab_size\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models() -> List[str]:\n",
    "    return [elem for elem in sorted(os.listdir(OUTPUT_DIR)) if not elem.startswith(\".\")]\n",
    "    \n",
    "models = get_models()\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966eb67",
   "metadata": {},
   "source": [
    "# 1. Show examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\N{ANGSTROM SIGN}\", \"\\N{LATIN CAPITAL LETTER A WITH RING ABOVE}\", \"\\u0041\\u030A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762de314",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = TEST_EXAMPLES + [\n",
    "    'Allmänna Allmänna',\n",
    "    \"<|endoftext|> test\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072271b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_hack(_decoded_elementwise):\n",
    "    \"\"\"\n",
    "    needs to be improved: \n",
    "    - should only be applied if add_prefix_space == True & add_whitespace_tokens == 24\n",
    "    - should only change an element if the next element is a non-whitespace-element\n",
    "    \"\"\"\n",
    "    return [\n",
    "        elem[:-1] \n",
    "        if set(elem) == {' '} \n",
    "        else elem\n",
    "        for elem in _decoded_elementwise \n",
    "    ]\n",
    "    # return \"\".join(decoded_elementwise_hack)\n",
    "\n",
    "def display(_example_decoded_per_token, show_linebreak = False):\n",
    "    newline = \"↩\\n\" if show_linebreak else \"↩\"\n",
    "    example_decoded_per_token = [\n",
    "            elem.replace(\"\\n\", newline).replace(\" \", \"-\")\n",
    "            for elem in _example_decoded_per_token\n",
    "        ]\n",
    "    \n",
    "    COLORS = [\"red\", \"blue\"]\n",
    "    for i, elem in enumerate(example_decoded_per_token):\n",
    "        print(colored(elem, COLORS[i%len(COLORS)]), end=\"\")\n",
    "    print()\n",
    "    print(f\"> {len(example_decoded_per_token)} tokens\")\n",
    "    print()\n",
    "\n",
    "def show_example_model(example, model, show_tokenization, verbose: bool = False):\n",
    "    _id = model.split(\"_\")[0]\n",
    "    \n",
    "    if isfile(join(OUTPUT_DIR, model, \"tokenizer.json\")):\n",
    "        library = \"HF\"\n",
    "        tokenizer_file = join(OUTPUT_DIR, model, \"tokenizer.json\")\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_file)\n",
    "        tokenizer_fast = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "        encoding = tokenizer_fast.encode(example)\n",
    "        example_encoded = tokenizer_fast.convert_ids_to_tokens(encoding)   \n",
    "        example_decoded = tokenizer_fast.decode(encoding)    \n",
    "        \n",
    "    elif isfile(join(OUTPUT_DIR, model, \"model.model\")):\n",
    "        library = \"SP\"\n",
    "        tokenizer_file = join(OUTPUT_DIR, model, \"model.model\")\n",
    "        sp = spm.SentencePieceProcessor(model_file=tokenizer_file)\n",
    "        encoding = sp.encode(example, out_type=int)\n",
    "        example_encoded = sp.encode(example, out_type=str)\n",
    "        example_decoded = sp.decode(example_encoded)\n",
    "\n",
    "    example_decoded_bytes = example_decoded.encode(\"utf-8\")\n",
    "\n",
    "    if library == \"HF\":\n",
    "        example_decoded_elementwise = [tokenizer_fast.decode(elem) for elem in encoding]\n",
    "    elif library == \"SP\":\n",
    "        example_decoded_elementwise = list()\n",
    "        idx_end = 0\n",
    "        for i, token in enumerate(example_encoded):\n",
    "            if i == 0 and token.startswith(\"▁\"):\n",
    "                _token = token[1:]\n",
    "            elif i > 0 and token.startswith(\"▁\"):\n",
    "                _token = token.replace(\"▁\", \" \")\n",
    "            else:\n",
    "                _token = token\n",
    "            # print(i, token, _token)\n",
    "            idx_start = example_decoded[idx_end:].find(_token) + idx_end\n",
    "            idx_end = idx_start + len(_token)\n",
    "            # print(idx_start, idx_end)\n",
    "            # print()\n",
    "            example_decoded_elementwise.append(example_decoded[idx_start: idx_end])\n",
    "                \n",
    "    example_decoded_elementwise_hack = decode_hack(example_decoded_elementwise)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"============ {model}\")\n",
    "        print(f\"example: '{example}'\")\n",
    "        # if library == \"HF\":\n",
    "        #     print(f\"\\npre-tok: {tokenizer.pre_tokenizer.pre_tokenize_str(example)}\")\n",
    "        # print(encoding)\n",
    "        print(f\"\\nencoded: {example_encoded} --- {len(example_encoded)}\")\n",
    "        print(f\"\\ndecoded: '{example_decoded}'\")\n",
    "        # print(f\"\\ndecoded as bytes: {example_decoded_bytes}\")\n",
    "        print(f\"\\ndecoded elementwise: {example_decoded_elementwise}\")\n",
    "        print()\n",
    "    \n",
    "    if show_tokenization: \n",
    "        print(\"\\nencoded:\")\n",
    "        display(example_encoded)\n",
    "        print(\"\\ndecoded:\")\n",
    "        display(example_decoded_elementwise, show_linebreak=True)\n",
    "        # if library == \"HF\":\n",
    "        #     print(\"\\ndecoded + hack:\")\n",
    "        #     display(example_decoded_elementwise_hack, show_linebreak=True)\n",
    "        print(f\"\\ndecoded = original: {example == example_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7464d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_examples(example=test_examples, model=[\"ALL\"] + models, show_tokenization=True, verbose=False):\n",
    "    if model == \"ALL\":\n",
    "        for model in sorted(models):\n",
    "            show_example_model(example, model, show_tokenization, verbose)\n",
    "    else:\n",
    "        show_example_model(example, model, show_tokenization, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff581883",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "\"ℌej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # ℌ, H --- ÅNGSTRÖM, Å, A+°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFC\n",
    "\"ℌej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # ℌ, H --- Å, Å, Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFKD\n",
    "\"Hej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # H, H --- A+°, A+°, A+°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFKC\n",
    "\"Hej Hej --- TVÅ TVÅ TVÅ\".encode(\"utf-8\")  # H, H --- Å, Å, Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960afd8a",
   "metadata": {},
   "source": [
    "# 2. Subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dfabe9",
   "metadata": {},
   "source": [
    "### 2a. Subword Length Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b5af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_histogram(model_1=models, model_2=[None] + models, xlim=20, ylim=15000):\n",
    "    plot_histogram(model_1, model_2, xlim, ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4345ff",
   "metadata": {},
   "source": [
    "### 2b. Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb91c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_compare_vocab(model_1=models, model_2=models, nr=30):\n",
    "    v, ex1, ex2 = compare_vocab(model_1, model_2, 1000000, 1000000)\n",
    "    print(v)\n",
    "    print()\n",
    "    print(\"=== only model 1 ===\")\n",
    "    print(ex1[:nr])\n",
    "    print()\n",
    "    print(\"=== only model 2 ===\")\n",
    "    print(ex2[:nr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79b08e",
   "metadata": {},
   "source": [
    "### 2c. Vocabulary Size & Subword Length Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab944411",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_vocab_size(model=models):\n",
    "    plot_vocab_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855cd98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937040ed",
   "metadata": {},
   "source": [
    "# 3. Multilinguality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc795db",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_multilinguality = [model for model in models if model.count(\"_3\") > 0]\n",
    "core = list(set([\"_\".join(model.split(\"_\")[1:-1]) for model in models_multilinguality if model.endswith(\"da\")]))[0]\n",
    "print(core)\n",
    "models_multilinguality = [model for model in models_multilinguality if core in model]\n",
    "models_multilinguality.sort(key = lambda x: x.split(\"_3\")[-1])\n",
    "models_multilinguality = {model.split(\"_3\")[-1]: model for model in models_multilinguality}\n",
    "models_multilinguality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = list(models_multilinguality.keys())\n",
    "lang_all = [l for l in lang if l.startswith(\"all\")]\n",
    "lang_pure = [l for l in lang if not l.startswith(\"all\")]\n",
    "\n",
    "models = {k: models_multilinguality[k] for k in lang}\n",
    "models_all = {k: models_multilinguality[k] for k in lang_all}\n",
    "models_pure = {k: models_multilinguality[k] for k in lang_pure}\n",
    "\n",
    "lang, lang_all, lang_pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview_corpus(models_multilinguality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08feeff2",
   "metadata": {},
   "source": [
    "### 3a. Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overview_data(models_pure.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c564e06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_overview(models_pure.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566e69a",
   "metadata": {},
   "source": [
    "### 3b. Intersection Matrix (Subword Length)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76a1bd07",
   "metadata": {},
   "source": [
    "def get_intersection_matrix(subword_length_threshold = None, normalize = True, plot = True):\n",
    "\n",
    "    lang = list(models_multilinguality.keys())\n",
    "    N = len(lang)\n",
    "    intersection_matrix = np.zeros([N, N])\n",
    "\n",
    "    for i, j in product(range(N), range(N)):\n",
    "        model_1 = models_multilinguality[lang[i]]\n",
    "        model_2 = models_multilinguality[lang[j]]\n",
    "\n",
    "        v, _, _ = compare_vocab(model_1, model_2, subword_length_threshold)\n",
    "        # print(lang_1, lang_2, v[\"intersection\"])\n",
    "        intersection_matrix[i, j] = v[\"intersection\"]\n",
    "    \n",
    "    # print(lang)\n",
    "    # print(intersection_matrix)\n",
    "    if normalize:\n",
    "        for i, j in product(range(N), range(N)):\n",
    "            if i != j:\n",
    "                intersection_matrix[i, j] = intersection_matrix[i, j] / intersection_matrix[i, i]\n",
    "        for i in range(N):\n",
    "            intersection_matrix[i, i] = 1.0\n",
    "    # print(intersection_matrix)\n",
    "    \n",
    "    if plot:\n",
    "        ax = sns.heatmap(intersection_matrix, \n",
    "                         xticklabels=lang,\n",
    "                         yticklabels=lang,\n",
    "                         cmap=\"binary\",\n",
    "                         vmin=0,\n",
    "                         annot=True,\n",
    "        )\n",
    "    else:\n",
    "        return intersection_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309cae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4cda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa03930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae785e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_intersection_matrix(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e35ab",
   "metadata": {},
   "source": [
    "### 3c. Intersection Timeline (Subword Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection(lang_1, lang_2, vocab_1, vocab_2):\n",
    "    model_1 = models_multilinguality[lang_1]\n",
    "    model_2 = models_multilinguality[lang_2]\n",
    "    v, _, _ = compare_vocab(model_1, model_2, vocab_1, vocab_2)\n",
    "    return v[\"intersection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4759a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_intersection('all-a1.0', 'da', 10000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e1589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = [10000, 20000, 30000, 40000, 51200, 64000, 80000, 96000, 112000, 128000]\n",
    "VOCAB_1 = VOCAB\n",
    "VOCAB_2 = VOCAB\n",
    "\n",
    "# VOCAB_1 = [50000, 100000, 150000, 200000, 250000]\n",
    "# VOCAB_2 = [100, 1000, 10000, 20000, 30000, 40000, 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e90e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections = {\n",
    "    lang_1: {\n",
    "        lang_2: {\n",
    "            vocab_1: {\n",
    "                vocab_2: get_intersection(lang_1, lang_2, vocab_1, vocab_2)\n",
    "                for vocab_2 in VOCAB_2\n",
    "            }\n",
    "            for vocab_1 in VOCAB_1\n",
    "        }\n",
    "        for lang_2 in lang\n",
    "    }\n",
    "    for lang_1 in lang_all\n",
    "}\n",
    "\n",
    "# intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelines_abs = {\n",
    "    lang_1: {\n",
    "        vocab_2: {\n",
    "            lang_2: \n",
    "            [intersections[lang_1][lang_2][vocab_1][vocab_2] for vocab_1 in VOCAB_1]\n",
    "            for lang_2 in lang_pure\n",
    "        }\n",
    "        for vocab_2 in VOCAB_2\n",
    "    }\n",
    "    for lang_1 in lang_all\n",
    "}\n",
    "# timelines_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c21e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelines_rel = {\n",
    "    lang_1: {\n",
    "        vocab_2: {\n",
    "            lang_2: \n",
    "            [intersections[lang_1][lang_2][vocab_1][vocab_2]/intersections[lang_1][lang_1][vocab_2][vocab_2] for vocab_1 in VOCAB_1]\n",
    "            for lang_2 in lang_pure\n",
    "        }\n",
    "        for vocab_2 in VOCAB_2\n",
    "    }\n",
    "    for lang_1 in lang_all\n",
    "}\n",
    "# timelines_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a72b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_timelines(tokenizer=lang_all, vocab_size=VOCAB_2):\n",
    "    lang_1 = tokenizer\n",
    "    vocab_2 = vocab_size\n",
    "    t_abs = timelines_abs[lang_1][vocab_2]\n",
    "    t_rel = timelines_rel[lang_1][vocab_2]\n",
    "    \n",
    "    plot_timelines(\n",
    "        VOCAB_1,\n",
    "        vocab_2,\n",
    "        [t_abs, t_rel],\n",
    "        lang_pure, \n",
    "        ylim=[1.1*100000, 1.1],\n",
    "        ylabel=[\"absolute\", \"relative\"], \n",
    "        title=[\"Coverage of single-language tokenizer vocabulary\"]*2,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-gpt-sw3-tokenizer",
   "language": "python",
   "name": "venv-gpt-sw3-tokenizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

"""Module that contains the Parameters class that contains all parameters for tokenizer training"""
import os
import csv
from typing import List
from os.path import join, isfile
import time
from src.env import Env
from src.helpers import LIST_OF_SPECIAL_TOKENS

env = Env()


class Parameters:
    """Class that contains all parameters for tokenizer training"""

    def __init__(
        self,
        library: str,
        tokenizer_name: str,
        dataset_files: List[str],
        dataset_filter: str = "all",
        unicode_normalization: str = "NFC",
        individual_digits: bool = True,
        add_prefix_space: bool = True,
        add_whitespace_tokens: int = 2,
        add_code_tokens: int = 1,
        add_newline_token: int = 0,
        minimum_frequency: int = 0,
        initial_alphabet: int = 1,
        byte_fallback: bool = True,
        character_coverage: float = 1.0,
        train_extremely_large_corpus: bool = True,
        vocab_size: int = 100,
        alpha: float = 1.0,
    ):
        assert library in [
            "HF",
            "SP",
        ], f"ERROR! library = {library} unknown, needs to be HF or SP"
        assert len(tokenizer_name), "ERROR! need to specify --tokenizer_name <str>"
        assert len(dataset_files), "ERROR need to specify --dataset_files <str>"
        self.library = library
        if dataset_files == ["all"]:
            self.dataset_files = _get_dataset_files_in_folder(
                env.data_train, dataset_filter
            )
        else:
            self.dataset_files = [
                join(env.data_train, dataset_file) for dataset_file in dataset_files
            ]
        self.tokenizer_name = tokenizer_name
        self.unicode_normalization = unicode_normalization
        self.individual_digits = bool(individual_digits)
        self.add_prefix_space = bool(add_prefix_space)
        self.add_whitespace_tokens = add_whitespace_tokens
        self.add_code_tokens = add_code_tokens
        self.add_newline_token = add_newline_token
        self.minimum_frequency = minimum_frequency
        self.initial_alphabet = initial_alphabet
        self.byte_fallback = bool(byte_fallback) if self.library == "SP" else 0
        self.character_coverage = character_coverage if self.library == "SP" else 0
        self.train_extremely_large_corpus = (
            bool(train_extremely_large_corpus) if self.library == "SP" else 0
        )
        self.vocab_size = vocab_size
        self.vocab_size_external = vocab_size
        self.alpha = alpha

        # DERIVED
        self.special_tokens: List[str] = []  # ["<|endoftext|>"]

        if self.add_whitespace_tokens == 1:
            whitespace_token = " " if self.library == "HF" else "▁"
            whitespace_tokens = [
                whitespace_token * i
                for i in range(2, 25)  # 2-24 consecutive whitespaces
            ]
            self.special_tokens += whitespace_tokens
        elif self.add_whitespace_tokens == 2:  # only self.library == "SP"
            assert (
                self.library == "SP"
            ), "ERROR! --add_whitespace_tokens 2 is only available for --library SP"
            self.vocab_size -= len(LIST_OF_SPECIAL_TOKENS)

        if self.add_code_tokens == 1:
            self.special_tokens += _get_code_tokens()

        if self.add_newline_token:
            self.special_tokens += "\n"

        self.timestamp = time.strftime("%Y%m%d_%H%M%S", time.localtime())[2:]

        suffix = (
            f"_{self.tokenizer_name}-a{self.alpha}"
            if self.alpha != -1
            else f"_{self.tokenizer_name}"
        )
        self.output_dir = join(env.output, self.timestamp) + self.get_id() + suffix

    def show(self) -> None:
        """print parameters"""
        print("=== PARAMETERS ===")
        print(f"> library = {self.library}")
        print(f"> unicode_normalization = {self.unicode_normalization}")
        print(f"> individual_digits = {self.individual_digits}")
        print(f"> add_prefix_space = {self.add_prefix_space}")
        print(f"> add_whitespace_tokens = {self.add_whitespace_tokens}")
        print(f"> add_code_tokens = {self.add_code_tokens}")
        print(f"> add_newline_token = {self.add_newline_token}")
        print(f"> minimum_frequency = {self.minimum_frequency}")
        print(f"> initial_alphabet = {self.initial_alphabet}")
        print(f"> byte_fallback = {self.byte_fallback}")
        print(f"> character_coverage = {self.character_coverage}")
        print(f"> train_extremely_large_corpus = {self.train_extremely_large_corpus}")
        print(f"> vocab_size = {self.vocab_size_external}")
        print(f"> alpha = {self.alpha}")
        print("==================")
        print(f"> special_tokens = {self.special_tokens}")
        print("==================")
        print()

    def get_id(self) -> str:
        """
        Returns:
            id string, e.g. '-v64000'
        """
        return f"-v{self.vocab_size_external}"


def _get_dataset_files_in_folder(_folder: str, _dataset_filter: str) -> List[str]:
    """
    get all dataset files in folder _folder that contain _dataset_filter as substring

    Args:
        _folder: e.g. 'data_train'
        _dataset_filter: 'file'

    Returns:
        files_in_folder: e.g. ['file1.jsonl', 'file2.jsonl']
    """
    print(f"> get files in {_folder}")
    _files = [
        elem
        for elem in os.listdir(_folder)
        if isfile(join(_folder, elem)) and elem.endswith(".jsonl")
    ]
    if len(_dataset_filter) and _dataset_filter != "all":
        _files = [elem for elem in _files if _dataset_filter in elem]
    assert len(
        _files
    ), f"ERROR! no files found in {_folder} (that contain '{_dataset_filter}')"
    return [join(_folder, elem) for elem in _files]


def _get_code_tokens() -> List[str]:
    with open("CODE_TOKENS.csv", newline="", encoding="utf-8") as csvfile:
        csvreader = csv.reader(csvfile, delimiter=",")
        code_tokens = list(csvreader)[0]
    return code_tokens

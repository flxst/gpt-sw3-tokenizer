{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Start","text":"<p>gpt-sw3-tokenizer - Train, evaluate and analyze BPE tokenizers.</p>"},{"location":"#resources","title":"Resources","text":"<ul> <li>source code: https://github.com/flxst/gpt-sw3-tokenizer</li> <li>documentation: https://flxst.github.io/gpt-sw3-tokenizer</li> <li>paper: https://arxiv.org/abs/2304.14780</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>git clone https://github.com/flxst/gpt-sw3-tokenizer.git\npip install -r requirements.txt\n</code></pre>"},{"location":"#about","title":"About","text":"<p>This repository provides easy-to-use tools to sample (weighted) data and subsequently train, evaluate and analyze a tokenizer. </p> Sampling Training Evaluation Analysis"},{"location":"#features","title":"Features","text":"<p> Sampling</p> <ul> <li>customizable amount of (disjunct) sampled data for training and evaluation</li> <li>weighting of different categories and languages</li> </ul> <p> Training</p> <ul> <li>support for SentencePiece and HuggingFace</li> <li>customizable tokenizer features (vocabulary size, handling of whitespace and numbers, ..)</li> </ul> <p> Evaluation </p> <ul> <li>computation of common tokenizer metrics (unknown rate, fertility, proportion of continued words, ..)</li> </ul> <p> Analysis</p> <ul> <li>example tokenization</li> <li>vocabulary overlap and performance comparison across languages</li> <li>effect of the vocabulary size</li> </ul>"},{"location":"#citation","title":"Citation","text":"<pre><code>@misc{gpt-sw3-tokenizer,\n  title = {Training and Evaluation of a Multilingual Tokenizer for {GPT}-{SW3}},\n  url = {http://arxiv.org/abs/2304.14780},\n  author = {Stollenwerk, Felix},\n  year = {2023},\n}\n</code></pre>"},{"location":"analysis/","title":"Analysis","text":""},{"location":"analysis/#how-to","title":"How-To","text":"<p>To analyze the trained tokenizers (and inspect the evaluation metrics), take the following steps:</p> <ul> <li> <p>start a jupyter notebook server</p> start jupyter notebook server <pre><code>cd notebook; jupyter notebook\n</code></pre> </li> <li> <p>open the notebook <code>tokenizer_analysis.ipynb</code> in your browser</p> </li> </ul>"},{"location":"analysis/#results","title":"Results","text":"<p>The notebook allows to examine e.g.</p> <ul> <li>tokenized examples</li> <li>evaluation metrics</li> <li>vocabulary and performance comparison across languages</li> <li>effect of the vocabulary size</li> </ul>"},{"location":"evaluation/","title":"Evaluation","text":""},{"location":"evaluation/#how-to","title":"How-To","text":"<p>To evaluate the tokenizer on data in the <code>&lt;data_eval&gt;</code> folder, run <code>script_evaluate.py</code>:</p> evaluate tokenizer <pre><code>python script_evaluate.py \n  --tokenizer_name &lt;tokenizer_name&gt;         # e.g. tokenizer1 \n  [--vocab_size &lt;vocab_size&gt;]               # e.g. 64000\n  [--monolingual]                           # if used, monolingual models are evaluated\n  [--vocab_size_pruned &lt;vocab_size_pruned&gt;] # e.g. 40000 51200 (only SP)\n</code></pre> <p>Arguments:</p> <ul> <li><code>--tokenizer_name</code> is the name of the tokenizer, e.g. <code>tokenizer1</code></li> </ul> <p> Optional Arguments:</p> <ul> <li><code>--vocab_size</code> specifies the vocabulary size of the tokenizer     (useful if there are several tokenizers with the same <code>tokenizer_name</code> but different vocabulary sizes)</li> <li><code>--monolingual</code> evaluates (multiple) monolingual tokenizers (trained with the same flag)</li> </ul> <p> Optional Arguments only available for SentencePiece:</p> <ul> <li><code>--vocab_size_pruned</code> evaluates the tokenizer with different, pruned vocabulary sizes </li> </ul> <p> The script</p> <ul> <li>applies the tokenizer <code>&lt;output&gt;/*_&lt;tokenizer_name&gt;</code> on each dataset <code>&lt;dataset_eval&gt;</code> in <code>&lt;data_eval&gt;</code></li> <li> <p>computes the following evaluation metrics (see the paper for details):</p> <ul> <li><code>unknown_rate</code></li> <li><code>fertility</code></li> <li><code>proportion_of_continued_words</code></li> <li><code>token_frequencies</code></li> </ul> </li> </ul>"},{"location":"evaluation/#results","title":"Results","text":"<p>Results are written to </p> <ul> <li><code>&lt;output&gt;/evaluation/results_&lt;tokenizer_name&gt;.json</code></li> <li><code>&lt;output&gt;/evaluation/token_frequencies_&lt;tokenizer_name&gt;.json</code></li> </ul> <p>They will be used for the analysis.</p>"},{"location":"preparation/","title":"Preparation","text":""},{"location":"preparation/#environment","title":"Environment","text":"<p>The environment file <code>env.ini</code> can be used to specify paths and settings. By default, it looks like this:</p> env.ini <pre><code>[main]\ndata_original = data_original\ndata_train = data_train\ndata_eval = data_eval\noutput = output\n\n[sampling]\nweights = SAMPLING_WEIGHTS.csv\n\n[other]\ndebug = 0\nverbose = 0\n</code></pre> <p>In the <code>[main]</code> section, the following folders are given: </p> <ul> <li><code>&lt;data_original&gt;</code>: contains original text data</li> <li><code>&lt;data_train&gt;</code>: contains sampled text data for training</li> <li><code>&lt;data_eval&gt;</code>: contains sampled text data for evaluation</li> <li><code>&lt;output&gt;</code>: contains trained tokenizers (incl. vocabulary, merge rules, parameters)</li> </ul> <p>The <code>[sampling]</code> section contains the path to the sampling weights file (see Sampling). The parameters in the <code>[other]</code> section should be 0 or 1 and can be used for debugging or verbose output. </p>"},{"location":"preparation/#data-format","title":"Data Format","text":"<ul> <li> <p>The files contained in the folder <code>&lt;data_original&gt;</code> must adhere to the following naming convention:   <pre><code>&lt;category&gt;_&lt;language&gt;.jsonl\n</code></pre>   Data split into multiple categories (e.g. <code>books</code>, <code>articles</code>, ..) and/or languages (e.g. <code>sv</code>, <code>da</code>, ..)   like this can be weighted in a customized way (as described in Sampling).</p> </li> <li> <p>Each row in a file needs to be formatted like this:   <pre><code>{\"text\": \"...\"} \n</code></pre>   Fields other than <code>\"text\"</code> may be present but will be ignored. </p> </li> </ul>"},{"location":"sampling/","title":"Sampling","text":"<p>Note: If you want to skip this step, just point the <code>&lt;data_train&gt;</code> to the <code>&lt;data_original&gt;</code> folder in the environment file and proceed with the training.</p> <p>Often times, especially in the case of very large datasets, one only wants to use a certain fraction of the original data for the tokenizer training and evaluation.  Moreover, the sampled training and evaluation data should be disjunct.  Finally, the data is sometimes weighted for tokenizer (and model) training, see e.g. GPT-3 or GPT-SW3.</p>"},{"location":"sampling/#how-to","title":"How-To","text":"<p>To sample (and weight) data from the original files in <code>&lt;data_original&gt;</code>, take the following steps:</p> <ul> <li> <p>Specify the categories, languages and their corresponding weights in <code>SAMPLING_WEIGHTS.csv</code>:</p> SAMPLING_WEIGHTS.csv <pre><code>category,sv,en\narticles,1,0.5\nbooks,0.7,1\n</code></pre> <p>The above example contains 2 categories (<code>articles</code> &amp; <code>books</code>) and 2 languages (<code>sv</code> &amp; <code>en</code>).</p> </li> <li> <p>Run <code>script_sampling.py</code>:</p> sample data <pre><code>python script_sampling.py \n    --percent &lt;percent&gt;   # e.g. 10\n    [--evaluation 0]      # 0 = &lt;data_train&gt;, 1 = &lt;data_eval&gt;\n</code></pre> <p>Arguments:</p> <ul> <li><code>--percent</code> is the fraction of documents with respect to original data in percent</li> </ul> <p> Optional Arguments:</p> <ul> <li><code>--evaluation</code> can be used to sample data for evaluation instead of training</li> </ul> </li> </ul> <p>Note that </p> <ul> <li>for each combination $x = cl$ of a category $c$ and language $l$, the fraction of sampled documents is given by the product of the individual weight $W_x$ read from <code>SAMPLING_WEIGHTS.csv</code> and the global factor $p$ specified via <code>--percent</code>:</li> </ul> <p>$$ \\left(\\frac{{\\text{size of sampled data}}}{{\\text{size of original data}}}\\right)_x \\approx \\left(\\frac{{\\text{number of sampled documents}}}{{\\text{number of original documents}}}\\right)_x = W_x \\cdot p  $$</p> <ul> <li>when evaluation data is sampled from the original data, the previously sampled training data is excluded in order to ensure disjunct samples </li> </ul>"},{"location":"sampling/#results","title":"Results","text":"<p>The sampled (and weighted) data files are called <code>&lt;category&gt;_&lt;language&gt;.jsonl</code> (just like their original counterparts, see preparation) and can be found in the folder</p> <ul> <li><code>&lt;data_train&gt;</code> if <code>--evaluation 0</code> is used</li> <li><code>&lt;data_eval&gt;</code> if <code>--evaluation 1</code> is used</li> </ul> <p>In addition, the folder contains a log file <code>SAMPLING.log</code> which contains information about the sampling process.</p> <p>In the next steps, the data are used for training and evaluation, respectively.</p>"},{"location":"training/","title":"Training","text":""},{"location":"training/#how-to","title":"How-To","text":"<p>To train a tokenizer on data in the <code>&lt;data_train&gt;</code> folder, run <code>script_train.py</code>:</p> train tokenizer <pre><code>python script_train.py \n  --tokenizer_name &lt;tokenizer_name&gt;      # e.g. tokenizer1\n  --dataset_files &lt;dataset_files&gt;        # e.g. \"all\" = all files in &lt;data_train&gt;\n  [--dataset_filter all]                 # e.g. \"all\" = no filter\n  [--vocab_size 64000]                   # int, divisible by 128\n  [--monolingual]                        # if used, monolingual models are trained\n  [--library SP]                         # SP = SentencePiece, HF = HuggingFace\n  [--unicode_normalization None]         # None, NFC, NFKC\n  [--individual_digits 1]                # 0, 1\n  [--add_prefix_space 1]                 # 0, 1\n  [--add_whitespace_tokens 2]            # 0, 1 (added at top of vocab), 2 (added at bottom of vocab)\n  [--add_code_tokens 1]                  # 0, 1 (added at top of vocab)\n  [--add_newline_token 0]                # 0, 1 (added at top of vocab)\n  [--minimum_frequency 0]                # int &gt;= 0\n  [--initial_alphabet 0]                 # 0, 1 (only HF)\n  [--byte_fallback 1]                    # 0, 1 (only SP)\n  [--character_coverage 0.9999]          # float, useful if byte_fallback = 1 (only SP)\n  [--train_extremely_large_corpus 1]     # 0, 1 (only SP)\n</code></pre> <p>Arguments:</p> <ul> <li><code>--tokenizer_name</code> will be the name of the tokenizer, e.g. <code>tokenizer1</code></li> <li><code>--dataset_files</code> specifies the data files in <code>&lt;data_train&gt;</code> you would like to include, e.g.<ul> <li><code>&lt;dataset_files&gt; = all</code> (which uses all files) or</li> <li><code>&lt;dataset_files&gt; = data_en.jsonl data_sv.jsonl</code>    (which uses two specific files)</li> </ul> </li> </ul> <p> Optional Arguments:</p> <ul> <li><code>--dataset_filter</code> is an alternative to <code>--dataset_files</code> (which needs to be set to <code>all</code>). Any files that contain the specified substring will be included</li> <li><code>--vocab_size</code> specifies the desired vocabulary size</li> <li><code>--monolingual</code> trains (multiple) monolingual tokenizers instead of a single multilingual one</li> <li><code>--library</code> specifies the library to use (<code>SP</code> = SentencePiece or <code>HF</code> = HuggingFace)</li> <li><code>--unicode_normalization</code> specifies the unicode normalization that the data is preprocessed with</li> <li><code>--individual_digits</code> splits digits into separate tokens using whitespace</li> <li><code>--add_prefix_space</code> adds dummy whitespace to beginning of first token of text</li> <li><code>--add_whitespace_tokens</code> adds 23 consecutive whitespace tokens to the tokenizer's vocabulary</li> <li><code>--add_code_tokens</code> adds special code tokens to the tokenizer's vocabulary. These are specified in <code>CODE_TOKENS.csv</code></li> <li><code>--add_newline_token</code> adds special <code>\"\\n\"</code> token to the tokenizer's vocabulary</li> <li><code>--minimum_frequency</code> specifies the minimum frequency required for a token to be added to the vocabulary   Note that this most likely results in a vocabulary size smaller than the vocabulary size specified beforehand</li> </ul> <p> Optional Arguments only available for HuggingFace:</p> <ul> <li><code>--initial_alphabet</code> adds 256 single characters to the tokenizer's vocabulary</li> </ul> <p>We refer to the HuggingFace documentation for further details.</p> <p> Optional Arguments only available for SentencePiece:</p> <ul> <li><code>--byte_fallback</code></li> <li><code>--character_coverage</code></li> <li><code>--train_extremely_large_corpus</code></li> </ul> <p>The corresponding SentencePiece features are used. We refer to the SentencePiece documentation for further details.</p>"},{"location":"training/#results","title":"Results","text":"<p>The trained tokenizer is saved at the folder <code>&lt;output&gt;/YYmmdd_HHMMSS-v&lt;vocab_size&gt;_&lt;tokenizer_name&gt;</code> and contains the following files:</p> <ul> <li> <p>Tokenizer: <code>model.model</code> &amp; <code>model.vocab</code> (if <code>library == \"SP\"</code>) or tokenizer.json (if <code>library == \"HF\"</code>)</p> model.vocab (SentencePiece) <pre><code>&lt;pad&gt; 0\n&lt;unk&gt; 0\n&lt;s&gt; 0\n&lt;|endoftext|&gt; 0\n&lt;|javascript|&gt; 0\n&lt;|python|&gt; 0\n&lt;|sql|&gt; 0\n&lt;|shell|&gt; 0\n&lt;0x00&gt; 0\n&lt;0x01&gt; 0\n\n[..]\n\n&lt;0xFE&gt; 0\n&lt;0xFF&gt; 0\n\u2581t -0\nhe -1\n\u2581a -2\n\n[..]\n\n\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 -63713\n</code></pre> tokenizer.json (HuggingFace) <pre><code>{\n    [..]\n    \"truncation\": null,\n    \"padding\": null,\n    \"added_tokens\": [\n        {\n            \"id\": 0,\n            \"content\": \"&lt;|javascript|&gt;\",\n            [..]\n            \"special\": true\n        },\n        {\n            \"id\": 1,\n            \"content\": \"&lt;|python|&gt;\",\n            [..]\n            \"special\": true\n        },\n        {\n            \"id\": 2,\n            \"content\": \"&lt;|sql|&gt;\",\n            [..]\n            \"special\": true\n        },\n        {\n            \"id\": 3,\n            \"content\": \"&lt;|shell|&gt;\",\n            [..]\n            \"special\": true\n        }\n    ],\n    \"normalizer\": null,\n    \"pre_tokenizer\": {\n        \"type\": \"Sequence\",\n        \"pretokenizers\": [\n            {\n                \"type\": \"ByteLevel\",\n                \"add_prefix_space\": true,\n                \"trim_offsets\": true,\n                \"use_regex\": true\n            },\n            {\n                \"type\": \"Digits\",\n                \"individual_digits\": true\n            }\n        ]\n    },\n    \"post_processor\": {\n        \"type\": \"ByteLevel\",\n        \"add_prefix_space\": true,\n        \"trim_offsets\": false,\n        \"use_regex\": true\n    },\n    \"decoder\": {\n        \"type\": \"ByteLevel\",\n        \"add_prefix_space\": true,\n        \"trim_offsets\": true,\n        \"use_regex\": true\n    },\n    \"model\": {\n    \"type\": \"BPE\",\n    \"dropout\": null,\n    \"unk_token\": null,\n    \"continuing_subword_prefix\": null,\n    \"end_of_word_suffix\": null,\n    \"fuse_unk\": false,\n    \"vocab\": {\n        \"&lt;|javascript|&gt;\": 0,\n        \"&lt;|python|&gt;\": 1,\n        \"&lt;|sql|&gt;\": 2,\n        \"&lt;|shell|&gt;\": 3,\n        \"!\": 4,\n        \"\\\"\": 5,\n\n        [..]\n\n        \"e on\",\n        \"e res\",\n        \"e ase\"\n    }\n}\n</code></pre> </li> <li> <p>Training parameters: <code>parameters.txt</code></p> parameters.txt <pre><code>library = SP\ndataset_files = [..]\ntokenizer_name = tokenizer1\nunicode_normalization = None\nindividual_digits = True\nadd_prefix_space = True\nadd_whitespace_tokens = 2\nadd_code_tokens = 1\nminimum_frequency = 0\nbyte_fallback = True\ncharacter_coverage = 0.9999\ntrain_extremely_large_corpus = True\nvocab_size = 63977\nvocab_size_external = 64000\nspecial_tokens = ['&lt;|javascript|&gt;', '&lt;|python|&gt;', '&lt;|sql|&gt;', '&lt;|shell|&gt;']\ntimestamp = 230912_110630\noutput_dir = [..]\n</code></pre> </li> <li> <p>Tokenizer Vocabulary Statistics: <code>tokenizer_subword_lengths.json</code></p> tokenizer_subword_lengths.json <pre><code>{\n    \"1\": 173,\n    \"2\": 1700, \n    \"3\": 4965, \n    \"4\": 8477, \n    [..]\n    \"24\": 1, \n    \"mean\": 6.656015625, \n    \"vocab_size\": 64000,\n}\n</code></pre> </li> <li> <p>Dataset Statistics: <code>overview.json</code></p> overview.json <pre><code>{\n    \"files\": 1, \n    \"documents_total\": 8625, \n    \"documents\": 8625, \n    \"dataset_files\": [..], \n    \"data_size_total\": \"0.0121G\", \n    \"data_size\": [\"0.0032G\", \"0.0022G\", \"0.0014G\", \"0.0004G\", \"0.0000G\", \"0.0038G\", \"0.0011G\"], \n    \"time\": \"7.95s\",\n}\n</code></pre> </li> </ul>"}]}